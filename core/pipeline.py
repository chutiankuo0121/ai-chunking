"""
Pipeline â€” åŸºäº asyncio.Queue çš„ç”Ÿäº§-æ¶ˆè´¹åˆ†å—ç®¡çº¿

ä¸‰é˜¶æ®µå¹¶å‘æµæ°´çº¿:
  Producer (Orchestrator) â†’ [Queue] â†’ Merger â†’ [Queue] â†’ Splitter â†’ [Queue] â†’ Consumer

è®¾è®¡å“²å­¦:
1. çœŸæµå¼: å—ä¸€æ—¦å°±ç»ªç«‹åˆ»ä¼ å…¥ä¸‹æ¸¸ï¼Œå‰ç«¯å®æ—¶å¯è§
2. èƒŒå‹æ§åˆ¶: Queue(maxsize) é™åˆ¶å†…å­˜å ç”¨ï¼Œä¸Šæ¸¸è‡ªåŠ¨é˜»å¡ç­‰å¾…
3. å“¨å…µç»ˆæ­¢: _SENTINEL å¯¹è±¡æ ‡è®°ç®¡çº¿ç»“æŸï¼Œå®‰å…¨å…³é—­
4. é˜¶æ®µè§£è€¦: æ¯ä¸ª Stage æ˜¯ç‹¬ç«‹ asyncio.Taskï¼Œå¯å•ç‹¬æµ‹è¯•/æ›¿æ¢
5. ç¡®å®šæ€§ä¿è¯: Merger + Splitter çº¯ç¡®å®šæ€§ï¼Œä¸è°ƒ LLM
"""

import asyncio
import re
import logging
from typing import Dict, List, AsyncGenerator

import tiktoken

logger = logging.getLogger("ai_chunking.core.pipeline")

# å“¨å…µå¯¹è±¡ â€” æ”¾å…¥é˜Ÿåˆ—è¡¨ç¤º "ä¸Šæ¸¸å·²å®Œæˆï¼Œæ— æ›´å¤šæ•°æ®"
_SENTINEL = object()


class ChunkPipeline:
    """
    å¼‚æ­¥ç”Ÿäº§è€…-æ¶ˆè´¹è€…åˆ†å—ç®¡çº¿ã€‚

    ç”¨æ³•::

        pipeline = ChunkPipeline(target_tokens=3500)
        async for chunk in pipeline.run(orchestrator_output):
            print(chunk)
    """

    def __init__(
        self,
        target_tokens: int,
        encoding_model: str = "cl100k_base",
        queue_size: int = 32,
    ):
        self.target_tokens = target_tokens
        self.min_tokens = target_tokens // 3  # è¿‡å°é˜ˆå€¼: 1/3 TARGET

        try:
            self.tokenizer = tiktoken.get_encoding(encoding_model)
        except Exception:
            self.tokenizer = tiktoken.get_encoding("cl100k_base")

        self._queue_size = queue_size

        # éªŒè¯è¿½è¸ªå™¨ â€” è®°å½•æ¯ä¸ªè¾“å‡º chunk çš„åˆè§„æ€§
        self._validation: Dict = {}
        self._reset_validation()

    def _reset_validation(self):
        """é‡ç½®éªŒè¯çŠ¶æ€ã€‚æ¯æ¬¡ run() è°ƒç”¨å‰è‡ªåŠ¨é‡ç½®ã€‚"""
        self._validation = {
            "passed": True,
            "total": 0,
            "oversized": [],    # [(chunk_id, tokens), ...]
            "undersized": [],   # [(chunk_id, tokens), ...]
            "min_tokens_seen": float("inf"),
            "max_tokens_seen": 0,
        }

    @property
    def validation(self) -> Dict:
        """è¿”å›æœ€è¿‘ä¸€æ¬¡ run() çš„éªŒè¯æŠ¥å‘Šã€‚"""
        return self._validation

    def _track_chunk(self, chunk: Dict):
        """è¿½è¸ªå•ä¸ª chunk çš„åˆè§„æ€§ã€‚"""
        v = self._validation
        tokens = chunk.get("tokens", 0)
        chunk_id = chunk.get("id", "?")

        v["total"] += 1
        v["min_tokens_seen"] = min(v["min_tokens_seen"], tokens)
        v["max_tokens_seen"] = max(v["max_tokens_seen"], tokens)

        if tokens > self.target_tokens:
            v["oversized"].append((chunk_id, tokens))
            v["passed"] = False
        elif tokens < self.min_tokens:
            v["undersized"].append((chunk_id, tokens))
            v["passed"] = False

    def _count_tokens(self, text: str) -> int:
        return len(self.tokenizer.encode(text))

    # ================================================================
    # Public API
    # ================================================================

    async def run(
        self, source: AsyncGenerator[Dict, None]
    ) -> AsyncGenerator[Dict, None]:
        """
        å¯åŠ¨ç®¡çº¿å¹¶æµå¼è¾“å‡ºæœ€ç»ˆ chunksã€‚

        ä¸‰ä¸ª Stage ä½œä¸º asyncio.Task å¹¶å‘è¿è¡Œ:
        - Producer: æ¶ˆè´¹ä¸Šæ¸¸ source â†’ raw_q
        - Merger:   raw_q â†’ merged_q (åˆå¹¶è¿‡å°å—)
        - Splitter: merged_q â†’ output_q (äºŒåˆ†è¿‡å¤§å—)

        ç®¡çº¿ç»“æŸåå¯é€šè¿‡ pipeline.validation è·å–éªŒè¯æŠ¥å‘Šã€‚
        """
        # é‡ç½®éªŒè¯çŠ¶æ€
        self._reset_validation()

        # æ¯æ¬¡ run åˆ›å»ºæ–°é˜Ÿåˆ— (æ”¯æŒé‡å¤è°ƒç”¨)
        raw_q: asyncio.Queue = asyncio.Queue(maxsize=self._queue_size)
        merged_q: asyncio.Queue = asyncio.Queue(maxsize=self._queue_size)
        output_q: asyncio.Queue = asyncio.Queue(maxsize=self._queue_size)

        # å¯åŠ¨ä¸‰é˜¶æ®µå¹¶å‘ä»»åŠ¡
        tasks = [
            asyncio.create_task(
                self._stage_producer(source, raw_q), name="pipeline-producer"
            ),
            asyncio.create_task(
                self._stage_merger(raw_q, merged_q), name="pipeline-merger"
            ),
            asyncio.create_task(
                self._stage_splitter(merged_q, output_q), name="pipeline-splitter"
            ),
        ]

        try:
            # ä»è¾“å‡ºé˜Ÿåˆ—æ¶ˆè´¹ â€” çœŸæµå¼ + é€å—éªŒè¯
            while True:
                chunk = await output_q.get()
                if chunk is _SENTINEL:
                    break
                self._track_chunk(chunk)
                yield chunk
        finally:
            # ç¡®ä¿æ‰€æœ‰ä»»åŠ¡æ­£å¸¸ç»“æŸ (å³ä½¿æ¶ˆè´¹æ–¹æå‰é€€å‡º)
            await asyncio.gather(*tasks, return_exceptions=True)

            # è¾“å‡ºéªŒè¯æ‘˜è¦æ—¥å¿—
            v = self._validation
            if v["passed"]:
                logger.debug(
                    f"âœ… éªŒè¯é€šè¿‡: {v['total']} ä¸ªå—å‡åœ¨ "
                    f"[{self.min_tokens}, {self.target_tokens}] tokens èŒƒå›´å†… "
                    f"(å®é™… {v['min_tokens_seen']}~{v['max_tokens_seen']})"
                )
            else:
                logger.warning(
                    f"ğŸš« éªŒè¯å¤±è´¥: {len(v['oversized'])} è¶…æ ‡, "
                    f"{len(v['undersized'])} è¿‡å° / å…± {v['total']} å—"
                )

    # ================================================================
    # Stage 0: Producer
    # ================================================================

    @staticmethod
    async def _stage_producer(
        source: AsyncGenerator[Dict, None],
        out_q: asyncio.Queue,
    ):
        """ä»ä¸Šæ¸¸ AsyncGenerator æ¶ˆè´¹ï¼Œé€ä¸ªæ”¾å…¥é˜Ÿåˆ—ã€‚"""
        try:
            async for chunk in source:
                await out_q.put(chunk)
        except Exception as e:
            logger.error(f"Producer å¼‚å¸¸: {e}")
        finally:
            await out_q.put(_SENTINEL)

    # ================================================================
    # Stage 1: Merger â€” åˆå¹¶è¿‡å°å—
    # ================================================================

    async def _stage_merger(
        self,
        in_q: asyncio.Queue,
        out_q: asyncio.Queue,
    ):
        """
        æµå¼åˆå¹¶å™¨ã€‚

        è§„åˆ™:
        - ç»´æŠ¤ä¸€ä¸ª buffer chunk
        - å¦‚æœ buffer.tokens < 1/3 TARGET â†’ åå¹¶ä¸‹ä¸€ä¸ª chunkï¼Œåˆå¹¶æ‘˜è¦
        - å¦‚æœ buffer åˆæ ¼ â†’ æ¨å…¥ä¸‹æ¸¸ï¼Œæ¢æ–° buffer
        - ç®¡çº¿ç»“æŸ â†’ flush buffer
        """
        buf = None

        try:
            while True:
                chunk = await in_q.get()

                if chunk is _SENTINEL:
                    if buf is not None:
                        await out_q.put(buf)
                    break

                if buf is None:
                    buf = chunk.copy()
                    continue

                if buf["tokens"] < self.min_tokens:
                    # buffer å¤ªå° â†’ åå¹¶ï¼Œåˆå¹¶æ‘˜è¦
                    buf["content"] += "\n\n" + chunk["content"]
                    buf["tokens"] = self._count_tokens(buf["content"])
                    buf["title"] += " & " + chunk["title"]
                    buf["summary"] = (
                        buf.get("summary", "") + " " + chunk.get("summary", "")
                    ).strip()
                    logger.debug(
                        f"Merged small chunk â†’ buffer now {buf['tokens']} tokens"
                    )
                    continue

                # buffer åˆæ ¼ â†’ æ¨å…¥ä¸‹æ¸¸
                await out_q.put(buf)
                buf = chunk.copy()

        except Exception as e:
            logger.error(f"Merger å¼‚å¸¸: {e}")
        finally:
            await out_q.put(_SENTINEL)

    # ================================================================
    # Stage 2: Splitter â€” äºŒåˆ†è¿‡å¤§å—
    # ================================================================

    async def _stage_splitter(
        self,
        in_q: asyncio.Queue,
        out_q: asyncio.Queue,
    ):
        """
        æµå¼æ‹†åˆ†å™¨ã€‚

        è§„åˆ™:
        - chunk.tokens > TARGET â†’ åœ¨ä¸­é—´å¥å­è¾¹ç•Œé€’å½’äºŒåˆ†
        - æ ‡æ³¨ "ï¼ˆç¬¬Néƒ¨åˆ†/å…±Méƒ¨åˆ†ï¼‰" åˆ°æ‘˜è¦
        - é™çº§é“¾: æ®µè½ > å¥å­ > æ¢è¡Œ > ç¡¬åˆ‡
        """
        counter = 0

        try:
            while True:
                chunk = await in_q.get()

                if chunk is _SENTINEL:
                    break

                if chunk["tokens"] > self.target_tokens:
                    logger.debug(
                        f"âœ‚ï¸ äºŒåˆ†: '{chunk['title'][:30]}' "
                        f"({chunk['tokens']} > {self.target_tokens})"
                    )
                    pieces = self._recursive_binary_split(
                        chunk["content"], self.target_tokens
                    )
                    total = len(pieces)
                    original_summary = chunk.get("summary", "")

                    for i, piece in enumerate(pieces, 1):
                        counter += 1
                        label = (
                            f"(Part {i}/{total}) " if total > 1 else ""
                        )
                        await out_q.put(
                            {
                                "id": f"CK-{counter:03d}",
                                "title": chunk["title"],
                                "summary": f"{label}{original_summary}",
                                "content": piece,
                                "tokens": self._count_tokens(piece),
                            }
                        )
                else:
                    counter += 1
                    chunk["id"] = f"CK-{counter:03d}"
                    await out_q.put(chunk)

        except Exception as e:
            logger.error(f"Splitter å¼‚å¸¸: {e}")
        finally:
            await out_q.put(_SENTINEL)

    # ================================================================
    # äºŒåˆ†æ‹†åˆ†å·¥å…·æ–¹æ³•
    # ================================================================

    def _recursive_binary_split(self, text: str, target: int) -> List[str]:
        """
        é€’å½’äºŒåˆ†ã€‚

        åœ¨ token ç»´åº¦çš„ä¸­é—´ä½ç½®æ‰¾æœ€è¿‘çš„å¥å­/æ®µè½è¾¹ç•Œåˆ‡ä¸€åˆ€ï¼Œ
        ç›´åˆ°æ¯æ®µ â‰¤ target_tokensã€‚
        """
        if self._count_tokens(text) <= target:
            return [text]

        tokens = self.tokenizer.encode(text)
        mid_char = len(self.tokenizer.decode(tokens[: len(tokens) // 2]))

        split_pos = self._find_middle_boundary(text, mid_char)

        left = text[:split_pos].strip()
        right = text[split_pos:].strip()

        # å®‰å…¨æ£€æŸ¥: å¦‚æœåˆ‡ä¸åŠ¨ï¼Œå›é€€åˆ°ç¡¬åˆ‡
        if not left or not right:
            return self._hard_split(text, target)

        return self._recursive_binary_split(
            left, target
        ) + self._recursive_binary_split(right, target)

    def _find_middle_boundary(self, text: str, mid: int) -> int:
        """
        åœ¨ mid é™„è¿‘æ‰¾æœ€è¿‘çš„æ®µè½/å¥å­/æ¢è¡Œè¾¹ç•Œã€‚

        æœç´¢èŒƒå›´: ä¸­é—´ Â± 25%ï¼Œä½†ä¿è¯ä¸¤è¾¹å„ â‰¥ 25% æ–‡æœ¬é•¿åº¦ã€‚
        ä¼˜å…ˆçº§: æ®µè½ > å¥å­ > æ¢è¡Œ > mid æœ¬èº«
        """
        total = len(text)
        margin = total // 4
        start = max(margin, mid - total // 4)
        end = min(total - margin, mid + total // 4)

        if start >= end:
            start, end = max(0, mid - 500), min(total, mid + 500)

        region = text[start:end]

        # æŒ‰ä¼˜å…ˆçº§æœç´¢: æ®µè½è¾¹ç•Œ â†’ å¥å­è¾¹ç•Œ â†’ æ¢è¡Œç¬¦
        for pattern in [r"\n\s*\n", r"[.!?ã€‚ï¼ï¼Ÿ]\s", r"\n"]:
            matches = [start + m.end() for m in re.finditer(pattern, region)]
            if matches:
                return min(matches, key=lambda p: abs(p - mid))

        return mid

    def _hard_split(self, text: str, target: int) -> List[str]:
        """æœ€åçš„æ‰‹æ®µ: æŒ‰ token æ•°ç¡¬åˆ‡ã€‚ä¿è¯ç»ä¸è¶…æ ‡ã€‚"""
        tokens = self.tokenizer.encode(text)
        return [
            self.tokenizer.decode(tokens[i : i + target])
            for i in range(0, len(tokens), target)
        ]
