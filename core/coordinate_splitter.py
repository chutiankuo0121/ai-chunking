"""
CoordinateSplitter â€” å”¯ä¸€çš„åæ ‡åˆ‡åˆ†å™¨

èŒè´£:
1. åœ¨æ–‡æœ¬ä¸­æ’å…¥åæ ‡æ ‡è®° (æ¯éš” ~interval tokens)
2. è°ƒç”¨ coordinate_split Tool è®© LLM å†³å®šè¯­ä¹‰åˆ‡å‰²ç‚¹
3. ä½¿ç”¨ CascadeMatcher å®šä½ LLM è¿”å›çš„ end_quote ä»¥ç‰©ç†åˆ‡å‰²æ–‡æœ¬
4. é€šè¿‡ ProtocolMemory æ³¨å…¥å…¨å±€ä¸Šä¸‹æ–‡ã€è®°å½•å‰æ–‡æ‘˜è¦
"""

import logging
import os
import re
from typing import List, Dict, Callable, Awaitable, Optional, AsyncGenerator

from pydantic import BaseModel, Field
import tiktoken

from .memory import ProtocolMemory
from .tools import coordinate_split, quick_summary, ChunkResult
from ..utils.matcher import CascadeMatcher

logger = logging.getLogger("ai_chunking.core")


# ============================================================
# æ•°æ®æ¨¡å‹
# ============================================================

class SplitterInput(BaseModel):
    """åˆ‡åˆ†å™¨çš„è¾“å…¥å‚æ•°ã€‚"""
    file_path: str = Field(..., description="ç›®æ ‡ markdown æ–‡ä»¶è·¯å¾„")
    target_tokens: int = Field(..., description="æ¯å—çš„ç›®æ ‡ tokens æ•°")
    max_llm_context: int = Field(..., description="LLM çš„æœ€å¤§ä¸Šä¸‹æ–‡çª—å£ (tokens)")


class ChunkMetadata(BaseModel):
    """åˆ‡åˆ†åæ¯ä¸ªå—çš„å…ƒæ•°æ®ã€‚"""
    chunk_id: str
    title: str
    summary: str
    synthetic_qa: list
    content: str
    length_tokens: int


# ============================================================
# CoordinateSplitter
# ============================================================

class CoordinateSplitter:
    """
    ä½¿ç”¨åæ ‡ç³»ç»Ÿçš„è¯­ä¹‰åˆ‡åˆ†å™¨ã€‚

    ç‰¹æ€§:
    1. åŸºäºåæ ‡çš„åˆ‡å‰²: ä½¿ç”¨åµŒå…¥çš„ [ID: N] æ ‡è®°è¿›è¡Œç²¾ç¡®åˆ‡å‰²ã€‚
    2. å…¨å±€ä¸Šä¸‹æ–‡æ„ŸçŸ¥: é€šè¿‡ ProtocolMemory è·å–æœ¯è¯­è¡¨ + å™äº‹å¤§çº² + å‰æ–‡æ‘˜è¦ã€‚
    3. Tool åŒ–çš„ LLM è°ƒç”¨: åˆ‡åˆ†ã€æ‘˜è¦ç­‰ LLM èƒ½åŠ›å…¨éƒ¨å°è£…åœ¨ tools.py ä¸­ã€‚
    4. æ··åˆé²æ£’æ€§: CascadeMatcher å¤„ç† LLM çš„ end_quote å®šä½ã€‚
    5. æ‰¹å¤„ç†: æŒ‰ batch å¤„ç†è¶…é•¿æ–‡æœ¬ã€‚
    """

    def __init__(
        self,
        llm_func: Optional[Callable[[str], Awaitable[str]]] = None,
        protocol_memory: Optional[ProtocolMemory] = None,
        encoding_model: str = "cl100k_base",
    ):
        self.llm_func = llm_func
        self.protocol_memory = protocol_memory

        try:
            self.tokenizer = tiktoken.get_encoding(encoding_model)
        except Exception:
            self.tokenizer = tiktoken.get_encoding("cl100k_base")

        self.matcher = CascadeMatcher()

    # â”€â”€ Token å·¥å…· â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _count_tokens(self, text: str) -> int:
        return len(self.tokenizer.encode(text))

    def _add_coordinates(self, text: str, interval: int) -> str:
        """æ¯éš” ~interval tokens æ’å…¥ [ID: 0], [ID: 1]..."""
        tokens = self.tokenizer.encode(text)
        total_tokens = len(tokens)

        parts = []
        current_idx = 0
        marker_id = 0

        while current_idx < total_tokens:
            end_idx = min(current_idx + interval, total_tokens)
            chunk_tokens = tokens[current_idx:end_idx]
            chunk_text = self.tokenizer.decode(chunk_tokens)

            parts.append(f"[ID: {marker_id}]")
            parts.append("\n" + chunk_text + "\n")

            marker_id += 1
            current_idx = end_idx

        return "".join(parts)

    # â”€â”€ å…¥å£ç‚¹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def split_file(self, params: SplitterInput) -> AsyncGenerator[ChunkMetadata, None]:
        """æµå¼åˆ‡åˆ†å…¥å£ç‚¹ã€‚é€å— yield ChunkMetadataã€‚"""
        # logger.info(f"ğŸš€ å¯åŠ¨åæ ‡åˆ‡åˆ†: {params.file_path}")
        logger.info("ğŸ­ æ™ºèƒ½åˆ†å—ä¸­... (Intelligent Chunking...)")
        logger.debug(f"Starting coordinate splitting: {params.file_path}")

        if not os.path.exists(params.file_path):
            raise FileNotFoundError(f"æ–‡ä»¶æœªæ‰¾åˆ°: {params.file_path}")

        with open(params.file_path, "r", encoding="utf-8") as f:
            full_text = f.read()

        async for c in self.split_text(full_text, params):
            meta = ChunkMetadata(
                chunk_id=str(c["id"]),
                title=c["title"],
                summary=c["summary"],
                synthetic_qa=c.get("synthetic_qa", []),
                content=c["content"],
                length_tokens=c["tokens"],
            )
            yield meta

    async def split_text(self, text: str, params: SplitterInput) -> AsyncGenerator[Dict, None]:
        """
        åˆ‡åˆ†æ–‡æœ¬ã€‚ç»è¿‡ä¸‰é˜¶æ®µå¼‚æ­¥ç®¡çº¿:
          Orchestrator â†’ [Queue] â†’ Merger â†’ [Queue] â†’ Splitter â†’ Consumer

        ç®¡çº¿ç‰¹æ€§:
        - çœŸæµå¼: å—å°±ç»ªå³è¾“å‡ºï¼Œå‰ç«¯å®æ—¶å¯è§
        - ç¡®å®šæ€§å…œåº•: è¿‡å°å—åˆå¹¶ã€è¿‡å¤§å—äºŒåˆ†ï¼Œçº¯ç¡®å®šæ€§ä¸è°ƒ LLM
        - èƒŒå‹æ§åˆ¶: asyncio.Queue(maxsize) è‡ªåŠ¨é™æµ
        - å®ŒæˆéªŒè¯: ç»“æŸåå¯é€šè¿‡ splitter.last_validation è·å–åˆè§„æŠ¥å‘Š
        """
        from .orchestrator import split_text as orchestrate_split
        from .pipeline import ChunkPipeline

        pipeline = ChunkPipeline(target_tokens=params.target_tokens)

        async for chunk in pipeline.run(orchestrate_split(self, text, params)):
            yield chunk

        # ç®¡çº¿å®Œæˆåï¼Œæš´éœ²éªŒè¯æŠ¥å‘Š
        self.last_validation = pipeline.validation

    # â”€â”€ è¾…åŠ©: å¿«é€Ÿæ‘˜è¦ (å§”æ‰˜ç»™ Tool) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def _quick_summary(self, text: str) -> Dict:
        """ä¸ºç»“æ„åŒ–ç›´æ¥ä¿ç•™çš„ Chunk ç”Ÿæˆæ ‡é¢˜å’Œæ‘˜è¦ã€‚"""
        if not self.llm_func:
            return {"title": "æ— æ ‡é¢˜", "summary": "æœªæä¾› LLM å‡½æ•°ã€‚"}
        return await quick_summary(text, self.llm_func)

    # â”€â”€ æ ¸å¿ƒ: åæ ‡åˆ‡åˆ† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def _process_large_block(
        self,
        text: str,
        params: SplitterInput,
        marker_interval: int,
        batch_size_tokens: int,
    ) -> AsyncGenerator[Dict, None]:
        """å¯¹è¶…è¿‡ target_tokens çš„å¤§æ–‡æœ¬å—æ‰§è¡Œåæ ‡åˆ‡åˆ†ã€‚"""
        if not self.llm_func:
            raise ValueError("æœªæä¾› LLM å‡½æ•°ã€‚")

        target_chunk_tokens = params.target_tokens

        tokens = self.tokenizer.encode(text)
        total_tokens = len(tokens)
        current_token_pos = 0

        while current_token_pos < total_tokens:
            # 1. å‡†å¤‡ batch
            batch_end_pos = min(current_token_pos + batch_size_tokens, total_tokens)

            # é¿å…æœ€åå‰©ä¸€å°æ®µ
            remaining_tokens = total_tokens - batch_end_pos
            if 0 < remaining_tokens < target_chunk_tokens // 3:
                batch_end_pos = total_tokens

            batch_tokens = tokens[current_token_pos:batch_end_pos]
            batch_text_raw = self.tokenizer.decode(batch_tokens)

            # 2. æ’å…¥åæ ‡æ ‡è®°
            marked_batch_text = self._add_coordinates(batch_text_raw, marker_interval)
            markers_per_chunk = max(target_chunk_tokens // marker_interval, 1)

            # 3. è·å–å…¨å±€ä¸Šä¸‹æ–‡
            global_context = "æ— å‰æ–‡è¯­å¢ƒã€‚"
            if self.protocol_memory:
                global_context = self.protocol_memory.get_running_context()

            # 4. è°ƒç”¨ coordinate_split Tool
            logger.debug(f"Processing batch {current_token_pos}-{batch_end_pos} tokens...")
            try:
                llm_results: List[ChunkResult] = await coordinate_split(
                    marked_text=marked_batch_text,
                    global_context=global_context,
                    marker_interval=marker_interval,
                    target_chunk_tokens=target_chunk_tokens,
                    markers_per_chunk=markers_per_chunk,
                    llm_func=self.llm_func,
                )
            except Exception as e:
                logger.error(f"coordinate_split Tool å¤±è´¥: {e}")
                raise e

            # 5. ç‰©ç†åˆ‡å‰² (CascadeMatcher)
            batch_offset = 0

            for res in llm_results:
                search_context = batch_text_raw[batch_offset:]

                quote_rel_idx = -1
                if res.end_quote and len(res.end_quote.strip()) > 5:
                    quote_rel_idx = self.matcher.find_anchor(search_context, res.end_quote)

                if quote_rel_idx == -1:
                    logger.warning(f"å¼•ç”¨æœªæ‰¾åˆ°: '{res.end_quote[:50]}...'. ä½¿ç”¨æ ‡è®°å›é€€ã€‚")
                    try:
                        m_id = int(re.search(r'\d+', res.end_marker).group())
                        approx_token_offset = m_id * marker_interval
                        slice_tokens = batch_tokens[:approx_token_offset]
                        end_char_idx = len(self.tokenizer.decode(slice_tokens))
                    except Exception:
                        end_char_idx = len(batch_text_raw)
                else:
                    end_char_idx = batch_offset + quote_rel_idx

                chunk_content = batch_text_raw[batch_offset:end_char_idx]

                if len(chunk_content.strip()) > 0:
                    yield {
                        "id": res.chunk_id,
                        "title": res.title,
                        "summary": res.summary,
                        "synthetic_qa": getattr(res, "synthetic_qa", []),
                        "content": chunk_content,
                        "tokens": self._count_tokens(chunk_content),
                    }

                    if self.protocol_memory and res.summary:
                        self.protocol_memory.add_chunk_summary(res.summary)

                batch_offset = end_char_idx

            # â”€â”€ å¤„ç†å‰©ä½™éƒ¨åˆ† â”€â”€
            if batch_offset < len(batch_text_raw):
                remaining = batch_text_raw[batch_offset:]
                remaining_tokens = self._count_tokens(remaining)

                if remaining_tokens > target_chunk_tokens:
                    # å¤§å—å‰©ä½™: é€’å½’åˆ‡åˆ† (Orchestrator ä¸è´Ÿè´£é€’å½’ï¼Œåªè´Ÿè´£åˆå¹¶é€»è¾‘)
                    logger.debug(f"Remainder {remaining_tokens} > target, recursive split...")
                    async for sub in self._process_large_block(
                        remaining, params, marker_interval, batch_size_tokens
                    ):
                        yield sub
                else:
                    # å°/ä¸­å‰©ä½™: æŠ›å‡ºç»™ Orchestrator å†³å®šæ˜¯å¦å¸é™„
                    # å…ˆä¸ç”¨ç”Ÿæˆ summary (å¦‚æœè¢«å¸é™„å°±æµªè´¹äº†), 
                    # ä½†å¦‚æœä¸å¸é™„(ä¸­å—)è¿˜æ˜¯éœ€è¦çš„ã€‚
                    # ç­–ç•¥: å…ˆä¸ç”Ÿæˆï¼Œç”± Orchestrator åœ¨å†³å®š"ä¸å¸é™„"æˆ–"äºŒåˆ†"æ—¶å†è¡¥æ•‘ç”Ÿæˆã€‚
                    yield {
                        "id": "remainder",
                        "title": "å¾…å®š",
                        "summary": "",
                        "content": remaining,
                        "tokens": remaining_tokens,
                        "_is_remainder": True, 
                    }

            current_token_pos = batch_end_pos
