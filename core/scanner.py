"""
ScannerEngine â€” Phase 1 å…¨å±€æ‰«æå¼•æ“

å¯¹æ–‡æ¡£æ‰§è¡Œå…¨å±€æ‰«æï¼Œæå–:
- æ–‡æ¡£æ¦‚è¿° (doc_overview) â€” çº¯æ–‡æœ¬ï¼Œé²æ£’
- å™äº‹å¤§çº² (Narrative) â€” ç»“æ„åŒ– JSON

ç»“æœå­˜å…¥ ProtocolMemoryï¼Œä¾› Phase 2 çš„ CoordinateSplitter ä½¿ç”¨ã€‚
"""

import asyncio
import logging
from typing import Callable, Awaitable, Optional

import tiktoken

from core.memory import ProtocolMemory, GlobalProtocol, load_prompt, _safe_fill
from core.tools import extract_narrative

logger = logging.getLogger("ai_chunking.core.scanner")


class ScannerEngine:
    """
    Phase 1 æ‰«æå¼•æ“ã€‚
    
    æµç¨‹:
    1. ç”Ÿæˆå…¨æ–‡æ¦‚è¿° (doc_overview) â€” çº¯æ–‡æœ¬ï¼Œä¸éœ€è¦ JSON è§£æ
    2. å¯¹åˆ†ç‰‡å¹¶è¡Œæå–å™äº‹å¤§çº² (narrative)
    3. åˆå¹¶åˆ° ProtocolMemory
    
    å‚æ•°:
        llm_func:         LLM è°ƒç”¨å‡½æ•°
        max_llm_context:  LLM ä¸Šä¸‹æ–‡çª—å£å¤§å°
        on_progress:      å¯é€‰çš„è¿›åº¦å›è°ƒ (ç”¨äº WebSocket æ¨é€)
    """

    def __init__(
        self,
        llm_func: Callable[[str], Awaitable[str]],
        max_llm_context: int,
        on_progress: Optional[Callable] = None,
        encoding_model: str = "cl100k_base",
    ):
        self.llm_func = llm_func
        # é™åˆ¶ chunk_size ä¸Šé™ä¸º 12kï¼Œé˜²æ­¢ output bottleneck å¯¼è‡´ç»†èŠ‚ä¸¢å¤±
        self.chunk_size = min(int(max_llm_context * 0.33), 12000)
        self.on_progress = on_progress
        
        try:
            self.tokenizer = tiktoken.get_encoding(encoding_model)
        except Exception:
            self.tokenizer = tiktoken.get_encoding("cl100k_base")

    def _count_tokens(self, text: str) -> int:
        return len(self.tokenizer.encode(text))

    def _split_into_chunks(self, text: str) -> list:
        """å°†æ–‡æœ¬æŒ‰ chunk_size token æ•°åˆ‡ç‰‡ã€‚"""
        tokens = self.tokenizer.encode(text)
        chunks = []
        for i in range(0, len(tokens), self.chunk_size):
            chunk_tokens = tokens[i:i + self.chunk_size]
            chunks.append(self.tokenizer.decode(chunk_tokens))
        return chunks

    async def _generate_overview(self, text: str) -> str:
        """ç”Ÿæˆæ–‡æ¡£æ¦‚è¿°"""
        template = load_prompt("doc_overview")
        # ä½¿ç”¨å…¨æ–‡ç”Ÿæˆæ¦‚è¿°
        overview_text = text
        prompt = _safe_fill(template, text=overview_text)
        
        try:
            overview = await self.llm_func(prompt)
            logger.debug(f"ğŸ“ Doc overview generated ({len(overview)} chars)")
            return overview.strip()
        except Exception as e:
            logger.warning(f"Doc overview generation failed: {e}")
            return ""

    async def scan(self, text: str, doc_name: str = "") -> ProtocolMemory:
        """
        æ‰§è¡Œå…¨å±€æ‰«æï¼Œè¿”å›å¡«å……å¥½çš„ ProtocolMemoryã€‚
        
        æµç¨‹:
        1. ç”Ÿæˆæ–‡æ¡£æ¦‚è¿° (çº¯æ–‡æœ¬)
        2. å°†æ–‡æœ¬åˆ‡ç‰‡ï¼Œå¯¹æ¯ç‰‡æå–å™äº‹å¤§çº²
        3. åˆå¹¶ç»“æœåˆ° ProtocolMemory
        """
        # logger.info(f"ğŸ” Scanner starting for '{doc_name}' ({self._count_tokens(text):,} tokens)")
        logger.info("ğŸ“„ AIæ€»ç»“æ–‡ç« ä¸­... (AI Summarizing Document...)")
        logger.debug(f"Scanner starting for '{doc_name}' ({self._count_tokens(text):,} tokens)")

        protocol = GlobalProtocol(doc_name=doc_name)
        memory = ProtocolMemory(protocol=protocol)

        # åˆ‡ç‰‡
        chunks = self._split_into_chunks(text)
        logger.debug(f"Split into {len(chunks)} chunks for scanning")

        # â”€â”€ å¹¶è¡Œ: æ–‡æ¡£æ¦‚è¿° + å™äº‹å¤§çº² â”€â”€
        async def scan_narrative_chunk(i: int, chunk: str):
            """å¯¹å•ä¸ª chunk æå–å™äº‹å¤§çº²"""
            logger.debug(f"Scanning chunk {i+1}/{len(chunks)}...")
            narrative_items = await extract_narrative(chunk, self.llm_func)
            return i, narrative_items

        # åŒæ—¶å‘å‡º: æ¦‚è¿° + æ‰€æœ‰ chunk çš„å™äº‹æå–
        overview_task = self._generate_overview(text)
        narrative_tasks = [scan_narrative_chunk(i, chunk) for i, chunk in enumerate(chunks)]
        
        all_results = await asyncio.gather(overview_task, *narrative_tasks)
        
        # ç¬¬ä¸€ä¸ªç»“æœæ˜¯æ¦‚è¿°
        doc_overview = all_results[0]
        narrative_results = all_results[1:]

        # è®¾ç½®æ¦‚è¿°
        memory.protocol.doc_overview = doc_overview
        if self.on_progress and doc_overview:
            await self.on_progress("protocol_update", {
                "category": "doc_overview",
                "content": doc_overview,
            })

        # â”€â”€ æŒ‰é¡ºåºåˆå¹¶å™äº‹ç»“æœ â”€â”€
        for i, narrative_items in sorted(narrative_results, key=lambda x: x[0]):
            for item in narrative_items:
                memory.add_narrative(item)

                if self.on_progress:
                    await self.on_progress("protocol_update", {
                        "category": "narrative",
                        "content": {
                            "section": item.section,
                            "goal": item.goal,
                        }
                    })

        stats = memory.stats()
        logger.debug(
            f"âœ… Scanner complete: "
            f"overview={'âœ“' if doc_overview else 'âœ—'}, "
            f"{stats['narrative_count']} sections"
        )

        return memory
