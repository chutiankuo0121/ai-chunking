# ai-chunking package init

import logging
from pathlib import Path
from typing import Callable, Awaitable, AsyncGenerator, Dict, Union

from core.coordinate_splitter import CoordinateSplitter, SplitterInput, ChunkMetadata
from core.memory import ProtocolMemory
from core.scanner import ScannerEngine
from core.tools import ChunkResult, coordinate_split, quick_summary, extract_narrative
from utils.matcher import CascadeMatcher
from utils.logger import setup_logger

# å‘åŽå…¼å®¹
ContextAwareSplitter = CoordinateSplitter

__all__ = [
    "CoordinateSplitter",
    "ContextAwareSplitter",
    "SplitterInput",
    "ChunkMetadata",
    "ChunkResult",
    "ProtocolMemory",
    "ScannerEngine",
    "CascadeMatcher",
    "coordinate_split",
    "quick_summary",
    "extract_narrative",
    "smart_chunk_file",  # New API
    "get_openai_wrapper" # New API Helper
]

async def smart_chunk_file(
    file_path: Union[str, Path],
    llm_func: Callable[[str], Awaitable[str]],
    target_tokens: int,
    max_llm_context: int,
) -> AsyncGenerator[Dict, None]:
    """
    ä¸€ç«™å¼æ™ºèƒ½åˆ‡åˆ† API (Unified Smart Chunking API).
    
    è‡ªåŠ¨æ‰§è¡Œä¸¤é˜¶æ®µå¤„ç†:
    1. Phase 1 (Scanner): å…¨æ–‡æ‰«æï¼Œç”Ÿæˆå¤§çº²ä¸Žæ‘˜è¦ (Global Scan & Narrative Extraction)
    2. Phase 2 (Splitter): åŸºäºŽä¸Šä¸‹æ–‡çš„æ™ºèƒ½åˆ‡åˆ† (Context-Aware Splitting)
    
    Args:
        file_path: ç›®æ ‡æ–‡ä»¶è·¯å¾„ (Markdown/Text)
        llm_func: å¼‚æ­¥ LLM è°ƒç”¨å‡½æ•° (prompt -> str response)
        target_tokens: æ¯ä¸ªå—çš„ç›®æ ‡ token æ•° (é»˜è®¤ 3500)
        max_llm_context: æ¨¡åž‹æœ€å¤§ä¸Šä¸‹æ–‡çª—å£ (é»˜è®¤ 128000)
        
    Yields:
        Dict: åŒ…å« chunk ä¿¡æ¯çš„å­—å…¸ (id, title, summary, content, tokens)
    """
    logger = logging.getLogger("ai_chunking.api") # Standard logger
    
    path_obj = Path(file_path)
    if not path_obj.exists():
        raise FileNotFoundError(f"File not found: {file_path}")
        
    logger.info(f"ðŸš€ Processing: {path_obj.name}")
    
    with open(path_obj, "r", encoding="utf-8") as f:
        text = f.read()

    # Phase 1: Scanner
    # Logs are handled internally ("AI Summarizing...")
    scanner = ScannerEngine(llm_func=llm_func, max_llm_context=max_llm_context)
    try:
        memory = await scanner.scan(text, doc_name=path_obj.name)
    except Exception as e:
        logger.error(f"Phase 1 (Scanning) failed: {e}")
        raise e

    # Phase 2: Splitting
    # Logs are handled internally ("Intelligent Chunking...")
    splitter = CoordinateSplitter(llm_func=llm_func, protocol_memory=memory)
    params = SplitterInput(
        file_path=str(path_obj.absolute()),
        target_tokens=target_tokens,
        max_llm_context=max_llm_context,
    )
    
    try:
        async for chunk in splitter.split_text(text, params):
            # Standardize output format
            yield {
                "chunk_id": chunk.get("id"),
                "title": chunk.get("title", "Untitled"),
                "summary": chunk.get("summary", ""),
                "content": chunk.get("content", ""),
                "tokens": chunk.get("tokens", 0),
            }
    except Exception as e:
        logger.error(f"Phase 2 (Splitting) failed: {e}")
        raise e
        
    logger.debug("smart_chunk_file iterator complete.")

def get_openai_wrapper(
    api_key: str,
    base_url: str = None,
    model: str,        # No default
    concurrency: int,  # No default
) -> Callable[[str], Awaitable[str]]:
    """
    åˆ›å»ºä¸€ä¸ªé¢„é…ç½®çš„ OpenAI LLM è°ƒç”¨å‡½æ•°ã€‚
    åŒ…å«: å¹¶å‘æŽ§åˆ¶ã€è‡ªåŠ¨é‡è¯•ã€ç©ºå“åº”å¤„ç†ã€‚
    
    Args:
        api_key: OpenAI API Key
        base_url: (Optional) Custom Base URL
        model: Model name
        concurrency: Max concurrent requests
        
    Returns:
        ç¬¦åˆ llm_func ç­¾åçš„å¼‚æ­¥å‡½æ•°
    """
    from openai import AsyncOpenAI
    from utils.api import llm_call, set_concurrency_limit

    # é…ç½®å…¨å±€å¹¶å‘
    set_concurrency_limit(concurrency)
    
    # åˆå§‹åŒ–å®¢æˆ·ç«¯
    client = AsyncOpenAI(api_key=api_key, base_url=base_url)
    
    async def wrapper(prompt: str, temperature: float = 0.7) -> str:
        return await llm_call(client, model, prompt, temperature)
        
    return wrapper
